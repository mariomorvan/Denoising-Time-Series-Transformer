{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc96a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl \n",
    "\n",
    "GPUS = int(torch.cuda.is_available())\n",
    "# torch.cuda.empty_cache() \n",
    "def print_cuda_summary():\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "\n",
    "    print(\"torch.cuda.get_device_properties(0).total_memory %fGB\"%(t/1024/1024/1024))\n",
    "    print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"Free (res - alloc) %fGB\"%(f/1024/1024/1024))\n",
    "\n",
    "if GPUS:\n",
    "    print_cuda_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 1000\n",
    "val_ratio = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "seed = 0\n",
    "standardise_axes = (0, 1)  # per sample standardisation\n",
    "\n",
    "num_workers = 0\n",
    "pin_memory = True\n",
    "\n",
    "if num_workers > 0:\n",
    "    import cv2\n",
    "    cv2.setNumThreads(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f62f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db97aff1",
   "metadata": {},
   "source": [
    "### TESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPUS:\n",
    "    path = \"/state/partition1/mmorvan/data/TESS/lightcurves/0001\"\n",
    "else:\n",
    "    path = \"/Users/mario/data/TESS/lightcurves/0027\"\n",
    "\n",
    "train_path = os.path.join(path, 'processed_train')\n",
    "test_path = os.path.join(path, 'processed_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d09a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split generation\n",
    "# import os\n",
    "# from datasets import TessDataset\n",
    "\n",
    "# test_ratio = 0.2\n",
    "    \n",
    "# dataset = TessDataset(path, \n",
    "#                       processed=True, \n",
    "#                       save=False\n",
    "#                       )\n",
    "\n",
    "# #dataset.n_dim = 1\n",
    "# # TRAIN/VAL SPLIT\n",
    "# test_size = int(test_ratio * len(dataset))\n",
    "# train_size = len(dataset) - test_size\n",
    "# print(train_size, test_size)\n",
    "# train_dataset, test_dataset = random_split(dataset, \n",
    "#                                           (train_size, test_size),\n",
    "#                                            generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "# for idx in train_dataset.indices:\n",
    "#     dataset.save_item(idx, train_path)\n",
    "# for idx in test_dataset.indices:\n",
    "#     dataset.save_item(idx, test_path)\n",
    "# #train_dataset.indices\n",
    "\n",
    "# %%bash\n",
    "# ls \"/state/partition1/mmorvan/data/TESS/lightcurves/0001/processed_test\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3da6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import TessDataset\n",
    "\n",
    "# test_dataset = TessDataset(test_path, load_processed=True)\n",
    "# train_dataset = TessDataset(train_path, load_processed=True)\n",
    "# len(test_dataset), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets.kepler_tess import TessDataset, Subset, split_indices\n",
    "\n",
    "from transforms import  Compose,StandardScaler, AddGaussianNoise, Mask, FillNans, RandomCrop, DownSample\n",
    "\n",
    "\n",
    "    \n",
    "transform_both_train = Compose([RandomCrop(800, exclude_missing_threshold=0.8),\n",
    "                          DownSample(2),\n",
    "                          Mask(0.3, block_len=None, value=None, exclude_mask=True),\n",
    "                          StandardScaler(dim=0),\n",
    "                          #FillNans(0),\n",
    "                         ])\n",
    "\n",
    "transform_both_2 = Compose([RandomCrop(800, exclude_missing_threshold=0.8),\n",
    "                               DownSample(2),\n",
    "#                                Mask(0.3, block_len=None, value=None, exclude_mask=True),\n",
    "                               StandardScaler(dim=0),\n",
    "                          #FillNans(0),\n",
    "                         ])\n",
    "\n",
    "\n",
    "\n",
    "transform = None\n",
    "\n",
    "if GPUS:\n",
    "    path = \"/state/partition1/mmorvan/data/TESS/lightcurves/0001\"\n",
    "else:\n",
    "    path = \"/Users/mario/data/TESS/lightcurves/0027\"\n",
    "\n",
    "dataset = TessDataset(train_path, \n",
    "                      load_processed=True, \n",
    "                      max_samples=max_samples,\n",
    "                      transform=transform,\n",
    "                      transform_both=transform_both_train,\n",
    "                      use_cache=True,\n",
    "                      )\n",
    "test_dataset = TessDataset(test_path, \n",
    "                           load_processed=True, \n",
    "                           use_cache=True,\n",
    "                          )\n",
    "\n",
    "\n",
    "#dataset.n_dim = 1\n",
    "# TRAIN/VAL SPLIT\n",
    "val_size = int(val_ratio * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_indices, val_indices = split_indices((train_size, val_size),\n",
    "                                           generator=torch.Generator().manual_seed(seed))\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "\n",
    "val_dataset1 = Subset(dataset, val_indices)\n",
    "val_dataset2 = Subset(dataset, val_indices, replace_transform_both=transform_both_2)\n",
    "\n",
    "test_dataset1 = Subset(test_dataset1, range(len(test_dataset1)), replace_transform_both=transform_both_train)\n",
    "test_dataset2 = Subset(test_dataset1, range(len(test_dataset1)), replace_transform_both=transform_both_2)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=num_workers, pin_memory=pin_memory)\n",
    "val_loader1 = DataLoader(val_dataset1, batch_size=batch_size, shuffle=False, \n",
    "                          num_workers=num_workers, pin_memory=pin_memory)\n",
    "val_loader2 = DataLoader(val_dataset2, batch_size=batch_size, shuffle=False, \n",
    "                          num_workers=num_workers, pin_memory=pin_memory)\n",
    "test_loader1 = DataLoader(test_dataset1, batch_size=batch_size, shuffle=False, \n",
    "                          num_workers=num_workers, pin_memory=pin_memory)\n",
    "test_loader2 = DataLoader(test_dataset2, batch_size=batch_size, shuffle=False, \n",
    "                          num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a741cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CollatePred(object):\n",
    "    # May need to pad outside to ensure the totatlity is contained in output :-) \n",
    "    def __init__(self, window, step=1, standardise=None):\n",
    "        self.window = window\n",
    "        self.step = step\n",
    "        self.scaler = StandardScaler(dim=1)\n",
    "        \n",
    "\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        x_out_list = []\n",
    "        y_out_list = []\n",
    "        m_out_list = []\n",
    "        info_out_list = []\n",
    "\n",
    "        seq_len = batch[0][0].shape[0]\n",
    "        d = (seq_len - window) / step + 1\n",
    "        if int(d) == d:\n",
    "            padding = 0\n",
    "        else:\n",
    "            padding = step\n",
    "            pad = nn.ConstantPad1d((0, padding), value=np.nan)\n",
    "                \n",
    "        for i in range(len(batch)):\n",
    "            x, y, m, info = batch[i]\n",
    "            \n",
    "            x_out = torch.tensor(x)\n",
    "            y_out = torch.tensor(y)\n",
    "            m_out = torch.tensor(m)\n",
    "            \n",
    "            if padding:\n",
    "                x_out = pad(x_out.T).T\n",
    "                y_out = pad(y_out.T).T\n",
    "                m_out = pad(m_out.T).T\n",
    "            \n",
    "            x_out = x_out.unfold(0, size=self.window, step=self.step).transpose(1,2)\n",
    "            y_out = y_out.unfold(0, size=self.window, step=self.step).transpose(1,2)\n",
    "            m_out = m_out.unfold(0, size=self.window, step=self.step).transpose(1,2)\n",
    "            \n",
    "            info_out = {k: torch.tensor([v]*len(x_out)) for k,v in info.items()}\n",
    "            \n",
    "            x_out = self.scaler.fit_transform(x_out)\n",
    "            y_out = self.scaler.transform(y_out)\n",
    "            info_out['left_crop'] = torch.arange(0, seq_len, step)\n",
    "            info_out['mu'] = self.scaler.centers\n",
    "            info_out['sigma'] = self.scaler.norms\n",
    "            \n",
    "            x_out_list += [x_out]\n",
    "            y_out_list += [y_out]\n",
    "            m_out_list += [m_out]\n",
    "            info_out_list += [info_out]\n",
    "        return (torch.cat(x_out_list),\n",
    "                torch.cat(y_out_list),\n",
    "                torch.cat(m_out_list),\n",
    "                {k: torch.cat([info_out_list[i][k] for i in range(len(batch))]) for k in info_out})\n",
    "\n",
    "loader_pred = DataLoader(test_dataset_predict, \n",
    "                        batch_size=1, \n",
    "                        shuffle=False, \n",
    "                        collate_fn=CollatePred(400, step=350),\n",
    "                        num_workers=num_workers, pin_memory=pin_memory)\n",
    "            \n",
    "X, Y, M, I = next(iter(loader_pred))  \n",
    "X.shape, Y.shape, M.shape\n",
    "# X.dtype, Y.dtype, M.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0026b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e45aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[:,25:-25].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f24c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stitching back\n",
    "# X_o = inverse_standardise_batch(X, I['mu'], I['sigma'])\n",
    "\n",
    "# plt.figure(figsize=(25,5))\n",
    "# for i in range(len(X)):\n",
    "#     x = X_o[i]\n",
    "#     plt.plot(range(I['left_crop'][i], I['left_crop'][i]+400),\n",
    "#              x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, m, meta = dataset[0]\n",
    "\n",
    "plt.plot(x)\n",
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881eecfc",
   "metadata": {},
   "source": [
    "## Noise Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779b287",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from utils import nanstd\n",
    "# X,_,_,_ = next(iter(train_loader))\n",
    "# B, L, D = X.shape\n",
    "\n",
    "\n",
    "# window = 10\n",
    "# n_windows = L // window\n",
    "# X.view(B, n_windows, window).shape\n",
    "\n",
    "# noise = nanstd(X.view(B, n_windows, window), -1, keepdim=True).nanmedian(1, keepdim=True).values\n",
    "\n",
    "# # Samples highlighted by non-white-noise contribution to the variance\n",
    "# plt.figure(figsize=(30,20))\n",
    "# for i in range(len(X)):\n",
    "#     plt.plot(X[i,:,0], alpha=1-noise[i,0,0].item()**1.5, lw=1/noise[i,0,0])\n",
    "# plt.title('Batch highlighted by inverse noise')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Distribution of noise estimates\n",
    "# plt.hist(noise[:,0,0].numpy(), 50)\n",
    "# plt.title('White Noise Level')\n",
    "# plt.show()\n",
    "\n",
    "# # After correction\n",
    "# plt.figure(figsize=(30,20))\n",
    "# plt.plot((X / noise)[:,:,0].T)\n",
    "# plt.show()\n",
    "\n",
    "# # Check of better noise estimate\n",
    "\n",
    "# def rolling_std(x, width=10):\n",
    "#     return pd.Series(x).rolling(width, center=True, min_periods=1).std().values\n",
    "\n",
    "# better_noise = []\n",
    "# for i in range(len(X)):\n",
    "#     better_noise += [np.nanmedian(rolling_std(X[i,:,0].numpy(), width=10))]\n",
    "# plt.scatter(noise, better_noise)\n",
    "\n",
    "# # dataset_temp = TessDataset(train_path)\n",
    "\n",
    "\n",
    "# # x, y, m, i = dataset_temp[np.random.randint(len(dataset_temp))]\n",
    "# # plt.plot((x - np.nanmedian(x))/np.nanmedian(x))\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62353c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.stats import estimate_noise\n",
    "\n",
    "# X,_,_,_ = next(iter(train_loader))\n",
    "\n",
    "# noise = estimate_noise(X, reduce='nanmedian')\n",
    "# # plt.hist(noise[9].flatten().numpy(), 50)\n",
    "\n",
    "# print(torch.isnan(X/noise).sum(), torch.isnan(X).sum())\n",
    "# # plt.plot((X/np.sqrt(noise))[:,:,0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8870d6",
   "metadata": {},
   "source": [
    "### Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f86f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import DummyDataset\n",
    "\n",
    "# dataset = DummyDataset(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be9fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y, m, meta = dataset[0]\n",
    "\n",
    "# plt.plot(x)\n",
    "# plt.plot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f4fc6",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf968e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ### TEST RUN\n",
    "# lit_model = LitImputer(1, noise_scaling=True)\n",
    "# trainer = pl.Trainer(max_epochs=10, gpus=GPUS)\n",
    "# result = trainer.fit(lit_model, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf3c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lit_model = LitImputer(1, noise_scaling=True)\n",
    "# from utils.stats import estimate_noise\n",
    "# from models.loss import MaskedMSELoss\n",
    "# criterion = MaskedMSELoss()\n",
    "\n",
    "# for X, Y, M, Info in train_loader:\n",
    "#     Y_pred = lit_model(X)\n",
    "#     noise = torch.sqrt(estimate_noise(Y))\n",
    "#     assert not noise.requires_grad\n",
    "#     assert not torch.isclose(noise, torch.zeros_like(noise), equal_nan=True).any()\n",
    "#     noise[torch.isnan(noise)] = 1.\n",
    "\n",
    "#     loss = criterion(Y_pred, Y, M)\n",
    "#     assert not torch.isnan(loss)\n",
    "    \n",
    "#     pred_proxy = Y_pred / noise\n",
    "#     Y_proxy = Y/noise\n",
    "# #     torch.isnan(Y_pred).sum(), torch.isnan(Y).sum(), torch.isnan(pred_proxy).sum(), torch.isnan(Y_proxy).sum()\n",
    "#     assert not torch.isinf(pred_proxy).any()\n",
    "#     loss_scaled = criterion(pred_proxy, Y_proxy, M)\n",
    "    \n",
    "#     assert not torch.isnan(loss_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d5ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TEST volume\n",
    "# from pytorch_lightning.loggers import NeptuneLogger\n",
    "\n",
    "# logger = NeptuneLogger(project=\"denoising-transformer\",\n",
    "#                            name='test')\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "\n",
    "# lit_model = LitImputer(n_dim=1, d_model=8, dim_feedforward=16, num_layers=1,\n",
    "# #                        attention='linear', seq_len=400,\n",
    "#                        random_ratio=1, zero_ratio=0., keep_ratio=0.,token_ratio=0, \n",
    "#                        #, token_ratio=0.8, \n",
    "#                        noise_scaling=\"true\",\n",
    "# #                        attention='linear', seq_len=400\n",
    "#                       )\n",
    "\n",
    "# trainer = pl.Trainer(max_epochs=10000, \n",
    "#                      gpus=GPUS,\n",
    "#                      logger=logger,\n",
    "#                      check_val_every_n_epoch=1)\n",
    "\n",
    "# result = trainer.fit(lit_model, \n",
    "#                      train_dataloaders=train_loader,\n",
    "#                      val_dataloaders=val_loader, \n",
    "#                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6e485",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import LitImputer\n",
    "torch.manual_seed(0)\n",
    "if GPUS:\n",
    "    print_cuda_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c249532",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitImputer(n_dim=1, d_model=64, dim_feedforward=128, lr=0.001,\n",
    "#                        attention='linear', seq_len=400,\n",
    "                       train_unit = 'noise', train_loss='mae'\n",
    "                       #, token_ratio=0.8, \n",
    "                       #noise_scaling=\"true\",\n",
    "#                        attention='linear', seq_len=400\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a27a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y, M, I = next(iter(train_loader))\n",
    "\n",
    "# X_masked, token_mask = lit_model.apply_mask(X, M)\n",
    "# X_masked[token_mask.bool()] = np.nan\n",
    "# # i = np.random.randint(len(X_masked))\n",
    "# plt.figure(figsize=(12,6))\n",
    "# #plt.scatter(range(400),X[i].detach(), s=50, alpha=0.6)\n",
    "# plt.scatter(range(400), X_masked[i].detach(), s=5, color='red')\n",
    "# plt.fill_between(range(len(M[i])), -3, 3, where=M[i].flatten(), alpha=0.3)\n",
    "# plt.fill_between(range(len(M[i])), -3, 3, where=token_mask[i].flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff3cb0",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb67f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "logger = NeptuneLogger(project=\"denoising-transformer\",\n",
    "                        name='tess_denoising',\n",
    "                       tags=[str(len(dataset))+' samples',\n",
    "                             #\"continued_from_den-186\"\n",
    "                             #\"large-training-set\",\n",
    "                             \"test fix Masked Loss\",\n",
    "                             \"train - \" + lit_model.train_unit,\n",
    "                             #\"star-scaled\"\n",
    "                        #      \"Noise2Noise\",\n",
    "                        #      \"Imputation\",\n",
    "                        #      \"cropped-800\",\n",
    "                        #      \"downsampled-2\",\n",
    "                        #      'linformer'\n",
    "                             #\"replaced_normal\",\n",
    "                             #\"Crop-400\", \n",
    "#                              f\"patch-{patch_size}\",\n",
    "#                              \"TPT\"\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f2af4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pytorch_lightning import seed_everything\n",
    "# seed_everything(1)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=5, \n",
    "                     logger=logger, \n",
    "                     gpus=GPUS,\n",
    "                     check_val_every_n_epoch=1)\n",
    "\n",
    "result = trainer.fit(lit_model, \n",
    "                     train_dataloaders=train_loader,\n",
    "                     val_dataloaders=[val_loader1, val_loader2], \n",
    "                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.fit(lit_model, \n",
    "                     train_dataloaders=train_loader,\n",
    "                     val_dataloaders=val_loader, \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cc90c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.test(lit_model, \n",
    "             dataloaders=test_loader\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8df9cef",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c489467",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "find . -mindepth 3 -maxdepth 3 -type d -name \"DEN-293\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # ckpt_path = \"/home/mmorvan/denoising-ts-transformer/.neptune/Tess-denoising17-02-2022_01-34-35/DEN-167/checkpoints/epoch=382-step=4992.ckpt\"\n",
    "# # ckpt_path = \"./.neptune/Tess-denoising_17-02-2022_02-19-37/DEN-170/checkpoints/epoch=1032-step=15494.ckpt\"\n",
    "# ckpt_path = \"./.neptune/Tess-denoising17-02-2022_11-11-26/DEN-186/checkpoints/epoch=601-step=15062.ckpt\"\n",
    "# ckpt_path = \"./.neptune/Tess-denoising17-02-2022_11-41-36/DEN-189/checkpoints/epoch=636-step=15924.ckpt\"\n",
    "# # ckpt_path = \"./.neptune/tess_denoising/DEN-219/checkpoints/epoch=2999-step=86999.ckpt\"\n",
    "# ckpt_path = './.neptune/tess_denoising/DEN-245/checkpoints/epoch=1382-step=20744.ckpt'\n",
    "#ckpt_path = \"./.neptune/tess_denoising/DEN-258/checkpoints/epoch=2999-step=44999.ckpt\"\n",
    "# ckpt_path = './.neptune/tess_denoising/DEN-264/checkpoints/epoch=2999-step=44999.ckpt'\n",
    "ckpt_path = \"./.neptune/tess_denoising/DEN-272/checkpoints/epoch=4999-step=74999.ckpt\"  # MAE\n",
    "#ckpt_path = \"./.neptune/tess_denoising/DEN-289/checkpoints/epoch=4999-step=74999.ckpt\"\n",
    "#ckpt_path = \"./.neptune/tess_denoising/DEN-294/checkpoints/epoch=4999-step=74999.ckpt\"\n",
    "#ckpt_path = \"./.neptune/tess_denoising/DEN-295/checkpoints/epoch=1696-step=25454.ckpt\"\n",
    "#ckpt_path = \"./.neptune/tess_denoising/DEN-296/checkpoints/epoch=4300-step=64514.ckpt\"\n",
    "ckpt_path = \"./.neptune/tess_denoising/DEN-299/checkpoints/epoch=4999-step=74999.ckpt\"\n",
    "lit_model = lit_model.load_from_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec91704",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa5a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cuda_summary()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.stats import estimate_noise\n",
    "from utils.postprocessing import compute_rollout_attention\n",
    "lit_model.eval().cuda()\n",
    "\n",
    "# del X, Y, M, I, AR\n",
    "with torch.no_grad():\n",
    "    X, Y, M, I = next(iter(loader_pred))\n",
    "    Y_pred = lit_model(X.cuda()).detach().cpu().numpy()\n",
    "    noise = estimate_noise(Y)\n",
    "    attention_maps = lit_model.get_attention_maps(X.cuda(), mask=M.cuda())\n",
    "    AR = compute_rollout_attention(attention_maps)\n",
    "(noise <= 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b2038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### # plot diagnostic\n",
    "from utils.postprocessing import plot_pred_diagnostic\n",
    "\n",
    "#### All indices in their order\n",
    "indices = range(len(X))\n",
    "step = 1\n",
    "\n",
    "##### Ordered selection wrt to noise contribution\n",
    "# n_plots = 8 \n",
    "# indices = np.argsort(noise.squeeze().detach().cpu())\n",
    "# step = len(indices)//n_plots\n",
    "# step = 2\n",
    "# step = 1\n",
    "\n",
    "\n",
    "indices = indices[::step]\n",
    "\n",
    "# i = np.argmin(noise.numpy().squeeze())\n",
    "# #i = np.argmax(estimate_noise(Y).numpy().squeeze())\n",
    "# i = np.random.randint(len(X))\n",
    "\n",
    "for i in indices:\n",
    "    x = X[i,:,0].detach().cpu().numpy()\n",
    "    y = Y[i,:,0].detach().cpu().numpy()\n",
    "    mask = M[i,:,0].detach().cpu().numpy()\n",
    "    info = {k:v[i].detach().cpu().item() for k,v in I.items()}\n",
    "    y_pred = Y_pred[i,:,0]\n",
    "    ar = AR[i].sum(1).detach().cpu().numpy()\n",
    "    plot_pred_diagnostic(x, y, y_pred, mask=mask, ar=ar, targetid=info['targetid'], mu=info['mu'], sigma=info['sigma'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Attention visu dev\n",
    "\n",
    "# # attention_maps = lit_model.get_attention_maps(X.cuda(), mask=M.cuda())\n",
    "# # ar = compute_rollout_attention(attention_maps)\n",
    "\n",
    "\n",
    "\n",
    "# plt.show()\n",
    "# plt.plot(ar)#.sum(0))\n",
    "\n",
    "# plt.show()\n",
    "# for l in range(len(attention_maps)):\n",
    "#     plt.plot(attention_maps[l][i].mean(0).detach().cpu())\n",
    "\n",
    "# chunk = range(290, 295)\n",
    "# attention_maps[0][i, chunk].shape\n",
    "# plt.show()\n",
    "# for l in range(len(attention_maps)):\n",
    "#     plt.plot(attention_maps[l][i, :, chunk].mean(-1).cpu().detach())\n",
    "# plt.show()\n",
    "# for l in range(len(attention_maps)):\n",
    "#     plt.plot(attention_maps[l][i, chunk].mean(0).cpu().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a37ed",
   "metadata": {},
   "source": [
    "### Plotting examples of attention\n",
    "Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot only attention - pred\n",
    "\n",
    "ncols = 4\n",
    "nrows = 4\n",
    "n = ncols * nrows\n",
    "indices = np.argsort(noise.squeeze().detach().cpu())\n",
    "step = len(indices)//n\n",
    "step = 2\n",
    "indices = indices[::step]\n",
    "\n",
    "fig, ax = plt.subplots(nrows, ncols, figsize=(10,5), sharex=True)\n",
    "fig.add_subplot(111, frameon=False)\n",
    "# hide tick and tick label of the big axes\n",
    "#fig.tight_layout(w_pad=-1)\n",
    "#plt.tight_layout(pad=0., w_pad=0.5, h_pad=1.0)\n",
    "plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "plt.grid(False)\n",
    "plt.ylabel('standardised flux')\n",
    "plt.xlabel('time steps')\n",
    "\n",
    "k = 0\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "        i = indices[k]\n",
    "        \n",
    "        x = X[i,:,0].detach().cpu().numpy()\n",
    "        y = Y[i,:,0].detach().cpu().numpy()\n",
    "        mask = M[i,:,0].detach().cpu().numpy()\n",
    "        info = {k:v[i].detach().cpu().item() for k,v in I.items()}\n",
    "        y_pred = Y_pred[i,:,0]\n",
    "        ar = AR[i].sum(1).detach().cpu().numpy()\n",
    "        \n",
    "#         ax[row, col].set_title('Prediction')\n",
    "#         ax[row, col].set_ylabel('stand. flux')\n",
    "\n",
    "        ma, Ma = np.min(ar), np.max(ar)\n",
    "        alpha = (ar-ma)/(Ma-ma)/1.002 + ma + 1e-5\n",
    "        s = (((ar-ma)/(Ma-ma)+ma)) * 20 + 1\n",
    "        \n",
    "        ax[row, col].scatter(range(len(x)), y, label='input',\n",
    "                         color='black', s=s, alpha=alpha)\n",
    "\n",
    "        ax[row, col].plot(y_pred, label='pred', color='red', lw=1, alpha=0.9)\n",
    "        ax[row, col].set_yticks([])\n",
    "        k += 1\n",
    "\n",
    "# hide tick and tick label of the big axes\n",
    "#plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "\n",
    "\n",
    "#ax[2,0].set_ylabel('standardised flux')\n",
    "#ax[0,0].set_xlabel('time steps')\n",
    "        \n",
    "import datetime\n",
    "date = datetime.datetime.now()\n",
    "plt.savefig(f'experiments/outputs/attention_preds_{date}.pdf', format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adabedaf",
   "metadata": {},
   "source": [
    "# Baselines and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ce7da",
   "metadata": {},
   "source": [
    "### Batch eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e2cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.stats import estimate_noise\n",
    "from utils.postprocessing import compute_rollout_attention\n",
    "lit_model.eval().cuda()\n",
    "\n",
    "# del X, Y, M, I, AR\n",
    "with torch.no_grad():\n",
    "    X, Y, M, I = next(iter(loader_pred))\n",
    "    Y_pred = lit_model(X.cuda()).cpu()\n",
    "    noise = estimate_noise(Y)\n",
    "    attention_maps = lit_model.get_attention_maps(X.cuda(), mask=M.cuda())\n",
    "    AR = compute_rollout_attention(attention_maps)\n",
    "(noise <= 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d694630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.postprocessing import inverse_standardise_batch, detrend\n",
    "from utils.stats import naniqr, compute_dw\n",
    "\n",
    "Y_o = inverse_standardise_batch(Y, I['mu'], I['sigma'])\n",
    "Y_pred_o = inverse_standardise_batch(Y_pred, I['mu'], I['sigma'])\n",
    "Y_d = detrend(Y_o, Y_pred_o).numpy()\n",
    "\n",
    "iqr_dtst = naniqr(Y_d, dim=1)\n",
    "dw_dtst = compute_dw(Y_d-1, reduce='none')\n",
    "iqr_dtst.mean(), np.median(iqr_dtst), np.abs(2-dw_dtst).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b2f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_dw(Y_d[0][~np.isnan(Y_d[0])]-1, axis=0), compute_dw(Y_d[0]-1, axis=0), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac1169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wotan\n",
    "\n",
    "def predict_batch_wotan(y, cadence=1/24, method='biweight', **kwargs):\n",
    "    # y in flux units!\n",
    "    y = y.detach().cpu().squeeze()\n",
    "    batch_size, len_seq = y.shape\n",
    "    \n",
    "    time = np.arange(len_seq) * cadence\n",
    "    list_flat = []\n",
    "    list_trend = []\n",
    "    for i in range(batch_size):\n",
    "        flattened_y, trend_y = wotan.flatten(time, y[i], method=method, return_trend=True, **kwargs)\n",
    "        list_flat += [flattened_y]\n",
    "        list_trend += [trend_y]\n",
    "        \n",
    "    return np.stack(list_flat), np.stack(list_trend)\n",
    "\n",
    "for window in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print('\\t', window)\n",
    "    Y_wotan_d, _ = predict_batch_wotan(Y_o, window_length=window)\n",
    "    iqr_wotan = naniqr(Y_wotan_d, dim=1)\n",
    "    dw_wotan = compute_dw(Y_wotan_d-1, reduce='none')\n",
    "    print(iqr_wotan.mean(), np.median(iqr_wotan),  np.abs(2-dw_wotan).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99608708",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((iqr_wotan - iqr_dtst.squeeze()), 100)\n",
    "plt.show()\n",
    "plt.hist((np.abs(2-dw_wotan) - np.abs(2-dw_dtst.squeeze())), 100)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_wotan_d, trend_wotan = predict_batch_wotan(Y_o, window_length=0.2)\n",
    "\n",
    "i = np.random.randint(len(Y))\n",
    "plt.plot(Y_o[i])\n",
    "plt.plot(trend_wotan[i])\n",
    "\n",
    "plot_pred_diagnostic(Y_o[i], Y_o[i], trend_wotan[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a485750",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(iqr_dtst.flatten(), 50, range=(0, 0.05))\n",
    "plt.hist(iqr_wotan.flatten(), 50, range=(0, 0.05))\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e4c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in indices:\n",
    "    x = X[i,:,0].detach().cpu().numpy()\n",
    "    y = Y[i,:,0].detach().cpu().numpy()\n",
    "    mask = M[i,:,0].detach().cpu().numpy()\n",
    "    info = {k:v[i].detach().cpu().item() for k,v in I.items()}\n",
    "    y_pred = Y_pred[i,:,0]\n",
    "\n",
    "    y_o = inverse_standardise_batch(y, info['mu'], info['sigma'])\n",
    "    y_pred_o = inverse_standardise_batch(y_pred, info['mu'], info['sigma'])\n",
    "    y_d = detrend(y_o, y_pred_o)\n",
    "    \n",
    "    time = np.arange(len(y_d)) / 24\n",
    "    y_d_biweight = wotan.flatten(time, y_o, method='biweight', return_trend=False)\n",
    "    y_d_medfilt = wotan.flatten(time, y_o, window_length=49, method='medfilt', return_trend=False)\n",
    "    \n",
    "    print(f'IQR(biweight) : {naniqr(y_d_biweight):.5f}')\n",
    "    print(f'IQR(medfilt) : {naniqr(y_d_medfilt):.5f}')\n",
    "    print(f'IQR(dtst) : {naniqr(y_d):.5f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b437c1",
   "metadata": {},
   "source": [
    "### Full-LC predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b6c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X, Y, M, I = next(iter(loader_pred))\n",
    "    Y_pred = lit_model(X.cuda()).cpu()\n",
    "    Y_o = inverse_standardise_batch(Y, I['mu'], I['sigma'])\n",
    "    Y_pred_o = inverse_standardise_batch(Y_pred, I['mu'], I['sigma'])\n",
    "    Y_d = detrend(Y_o, Y_pred_o).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access original TS non transformed \n",
    "idx = I['idx'][0]\n",
    "Y_intact = test_dataset.get_pretransformed_sample(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01437d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_back(x, skip=0, seq_len=None):\n",
    "    # Assumes no skip at the start\n",
    "    if skip == 0:\n",
    "        out = x.flatten()\n",
    "    else:\n",
    "        out = [x[0,:skip].flatten(), x[:,skip:-skip].flatten()]\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            out = torch.cat(out)\n",
    "        elif isinstance(x, np.ndarray):\n",
    "            out = np.concatenate(out)\n",
    "    if seq_len is not None:\n",
    "        out = out[:seq_len]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check same as inverse transfo \n",
    "skip = 25\n",
    "seq_len = len(Y_intact)\n",
    "Y_of = fold_back(Y_o, skip, seq_len=seq_len)\n",
    "assert np.isclose(Y_intact.flatten(), Y_of.flatten(), equal_nan=True).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show full pred with wotan pred too\n",
    "\n",
    "Y_pred_of = fold_back(Y_pred_o, skip, seq_len=seq_len)\n",
    "\n",
    "Y_wotan_d, trend_wotan_of = wotan.flatten(np.arange(seq_len) / 48,\n",
    "                                          Y_of,\n",
    "                                          window_length=0.5,\n",
    "                                          return_trend = True\n",
    "                                          )\n",
    "plt.figure(figsize=(25,10))\n",
    "plt.plot(Y_of)\n",
    "plt.plot(Y_pred_of)\n",
    "plt.plot(trend_wotan_of)\n",
    "# plt.plot(Y_of/Y_pred_of)\n",
    "#plt.xlim(3000,7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d750c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "#naniqr(Y_wotan_d), compute_dw(Y_wotan_d-1, 0)\n",
    "#naniqr(Y_of/Y_pred_of), compute_dw((Y_of- Y_pred_of).numpy(), 0),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bddea92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Produce full predictions\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "d_pred = {'dtst': [],\n",
    "          'biweight_0.5': [],\n",
    "          'medfilt': [],\n",
    "            }\n",
    "d_time = {'dtst': [],\n",
    "          'biweight_0.5': [],\n",
    "          'medfilt': [],\n",
    "            }\n",
    "\n",
    "k = 0\n",
    "for X, Y, M, I in tqdm(loader_pred):\n",
    "    k+=1\n",
    "#     if k> 20:\n",
    "#         break\n",
    "    # access original TS non transformed \n",
    "    idx = I['idx'][0]\n",
    "    Y_intact = test_dataset.get_pretransformed_sample(idx).squeeze()\n",
    "    seq_len = len(Y_intact)\n",
    "    time = np.arange(seq_len) / 48  # Get the actual time vector maybe\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        t0 = datetime.now()\n",
    "        Y_pred = lit_model(X.cuda()).cpu()\n",
    "        Y_pred_o = inverse_standardise_batch(Y_pred, I['mu'], I['sigma'])\n",
    "        Y_pred_of = fold_back(Y_pred_o, skip=25, seq_len=seq_len)\n",
    "        d_pred['dtst'] += [Y_pred_of]\n",
    "        d_time['dtst'] += [datetime.now()-t0]\n",
    "        \n",
    "        t0 = datetime.now()\n",
    "        d_pred['biweight_0.5'] += [wotan.flatten(time, Y_intact, window_length=0.5, \n",
    "                                                 method='biweight', return_trend = True)[1]]\n",
    "        d_time['biweight_0.5'] += [datetime.now()-t0]\n",
    "\n",
    "        t0 = datetime.now()\n",
    "        d_pred['medfilt'] += [wotan.flatten(time, Y_intact, window_length=49, \n",
    "                                            method='medfilt', return_trend = True)[1]]\n",
    "        d_time['medfilt'] += [datetime.now() - t0]\n",
    "\n",
    "for model in d_pred:\n",
    "    if len(d_pred[model]):\n",
    "        d_pred[model] = np.vstack(d_pred[model])\n",
    "        \n",
    "for model in d_time:\n",
    "    if len(d_time[model]):\n",
    "        d_time[model] = np.mean(d_time[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db41b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "\n",
    "target_test = np.vstack([test_dataset.get_pretransformed_sample(idx).squeeze() for idx in range(len(test_dataset))])\n",
    "iqr = dict()\n",
    "dw = dict()\n",
    "\n",
    "for model in ['dtst', 'biweight_0.5', 'medfilt']:\n",
    "    pred_d = target_test / d_pred[model]\n",
    "    iqr[model] = naniqr(pred_d, dim=1, reduction='mean')\n",
    "    dw[model] = np.abs(compute_dw(pred_d-1, axis=1, reduce='none')-2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7db847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc503c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25ed505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8680264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick median filter\n",
    "dim = 1\n",
    "window_size = 50\n",
    "step= 1\n",
    "from utils.stats import nanstd\n",
    "torch.nanmedian(X.unfold(dim, window_size, step),-1).values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42217a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e73fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_filter(x, width=35):\n",
    "    return pd.Series(x).rolling(width, center=True, min_periods=1).median().values\n",
    "\n",
    "median_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d80367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.loss import \n",
    "\n",
    "def median_filter(x, width=35):\n",
    "    return pd.Series(x).rolling(width, center=True, min_periods=1).median().values\n",
    "\n",
    "def mean_filter(x, width=10):\n",
    "    return pd.Series(x).rolling(width, center=True, min_periods=1).mean().values\n",
    "\n",
    "\n",
    "pred_mean = dict()\n",
    "res_mean = dict()\n",
    "res_median = dict()\n",
    "\n",
    "pred_median = dict()\n",
    "for w in (10, 25, 40, 55):\n",
    "    pred_mean[w] = np.vstack([mean_filter(x[:,0], w) for x in X])\n",
    "    res_mean[w] = pred_mean[w] - Y[i,:,0].detach().numpy()\n",
    "    pred_median[w] = np.vstack([median_filter(x[:,0], w) for x in X])\n",
    "    res_median[w] = pred_median[w] - Y[i,:,0].detach().numpy()\n",
    "\n",
    "    # i = np.random.randint(len(Y))\n",
    "    # plt.plot(X[i])\n",
    "    # plt.plot(Y[i])\n",
    "    # plt.plot(pred[i])\n",
    "\n",
    "    mse_median = MaskedMSELoss()(torch.tensor(pred_median[w]), Y[:,:,0]).item()\n",
    "    mse_mean = MaskedMSELoss()(torch.tensor(pred_mean[w]), Y[:,:,0]).item()\n",
    "    print(f'\\twindow = {w}')\n",
    "    print(f'median filter : {mse_median:.4f}')\n",
    "    print(f'mean filter : {mse_mean:.4f}')\n",
    "\n",
    "mse_tst = MaskedMSELoss()(torch.tensor(pred2[:,:,0], device=\"cpu\"), Y[:,:,0]).item()\n",
    "print(f'\\nTransformer : {mse_tst:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019cd883",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,6))\n",
    "plt.scatter(range(len(target)), target, label='target', color='green', alpha=0.7, s=5)\n",
    "#plt.plot(pred, label='Transformer', color='red', alpha=0.7)\n",
    "for w in pred_mean:\n",
    "    plt.plot(pred_mean[w][i], label=f'mean filter-{w}', alpha=0.7)\n",
    "\n",
    "plt.plot(pred, label='Transformer', lw=2)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(13,6))\n",
    "for w in [40]:\n",
    "    plt.scatter(range(len(pred_mean[w][i])), \n",
    "                pred_mean[w][i]-Y[i,:,j].detach().numpy(), label=f'mean filter-{w}', alpha=0.7)\n",
    "#     plt.scatter(range(len(res)), res, color='blue')\n",
    "\n",
    "plt.scatter(range(len(res)), res, color='red')\n",
    "\n",
    "plt.scatter(range(len(res)), res_m, marker=\"s\", color='red', label='transfo')\n",
    "#     plt.scatter(range(len(res)), res_m, marker=\"s\", color='blue')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.hist(res.numpy(), 50, range=(-3,3), label='transformer')\n",
    "plt.hist(res_mean[w][i], 50, range=(-3,3), label=f'mean filter-{w}', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,6))\n",
    "plt.scatter(range(len(target)), target, label='target', color='green', alpha=0.7, s=5)\n",
    "#plt.plot(pred, label='Transformer', color='red', alpha=0.7)\n",
    "for w in pred_median:\n",
    "    plt.plot(pred_median[w][i], label=f'median filter-{w}', alpha=0.7)\n",
    "\n",
    "plt.plot(pred, label='Transformer')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(13,6))\n",
    "for w in [40]:\n",
    "    plt.scatter(range(len(pred_median[w][i])), \n",
    "                pred_median[w][i]-Y[i,:,j].detach().numpy(), label=f'median filter-{w}', alpha=0.7)\n",
    "#     plt.scatter(range(len(res)), res, color='blue')\n",
    "\n",
    "\n",
    "plt.scatter(range(len(res)), res_m, marker=\"s\", color='red', label='transfo')\n",
    "#     plt.scatter(range(len(res)), res_m, marker=\"s\", color='blue')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.hist(res.numpy(), 50, range=(-3,3), label='transformer')\n",
    "plt.hist(res_median[w][i], 50, range=(-3,3), label=f'median filter-{w}', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938dfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean[w].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2036dcdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5e12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dcdc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f42b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5927ef4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afeaa31b",
   "metadata": {},
   "source": [
    "### Visualise attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e44d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e334d6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89e511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # size study\n",
    "list_seq_len = [100, 200,  400, 700, 1000, 2000]\n",
    "# for L in list_seq_len:\n",
    "#     torch.cuda.empty_cache()\n",
    "#     lit_model = LitImputer(n_dim, d_model=64, dim_feedforward=128,  num_layers=3, eye=eye, lr=0.001,\n",
    "#                        normal_ratio=0.2, keep_ratio=0., token_ratio=0.8, attention='linear', seq_len=L\n",
    "#                       )\n",
    "    \n",
    "#     dataset.transform_both = Compose([RandomCrop(L),\n",
    "#                                       StandardScaler(dim=0),\n",
    "#                                      ])\n",
    "#     train_loader = DataLoader(dataset, batch_size=64, shuffle=True) \n",
    "\n",
    "#     trainer = pl.Trainer(max_epochs=3, \n",
    "#                          gpus=1, profiler='simple')\n",
    "\n",
    "#     result = trainer.fit(lit_model, \n",
    "#                          train_dataloaders=train_loader,                     \n",
    "#                          )\n",
    "\n",
    "# result_full = [0.16705 , 0.24147, 0.51497, np.nan, np.nan, np.nan] #B256\n",
    "result_full = [0.2 , 0.27, 0.54, 1.2, np.nan, np.nan] #B64\n",
    "# result_prob = [ 0.30687, 0.33286, 0.38871, 0.54279, 0.68638, 1.1525 ] #B256\n",
    "result_prob = [0.47, 0.48, 0.53, 0.64, 0.81, 1.3] #B64\n",
    "\n",
    "###result_lin = [0.25, 0.31, 0.46, 0.65, 0.87, 1.53] #B64\n",
    "result_lin = [0.21, 0.24, 0.32, 0.42, 0.52, 0.87]\n",
    "plt.plot(list_seq_len, result_full, marker='o', label='Full attention B64')#, marker='-o')\n",
    "plt.plot(list_seq_len, result_prob, marker='o', label='Prob attention B64')\n",
    "plt.plot(list_seq_len, result_lin, marker='o', label='Linformer attention B64')\n",
    "plt.legend()\n",
    "plt.xlabel('Sequence length')\n",
    "plt.ylabel('Training epoch time')\n",
    "print('batch size', train_loader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ffe57c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacbf968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cdd91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca633b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dfacdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd2053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e6459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869dfdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ae4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10b39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc090a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e1c08302b48d315bee1cf8c8a590d46ae916d527a5787d1c0b0275f91d47b8b"
  },
  "kernelspec": {
   "display_name": "Python [conda env:dtst]",
   "language": "python",
   "name": "conda-env-dtst-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
