{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150518e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl \n",
    "\n",
    "GPUS = int(torch.cuda.is_available())\n",
    "# torch.cuda.empty_cache() \n",
    "def print_cuda_summary():\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "\n",
    "    print(\"torch.cuda.get_device_properties(0).total_memory %fGB\"%(t/1024/1024/1024))\n",
    "    print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"Free (res - alloc) %fGB\"%(f/1024/1024/1024))\n",
    "\n",
    "if GPUS:\n",
    "    print_cuda_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b3dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 1000\n",
    "val_ratio = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "seed = 0\n",
    "standardise_axes = (0, 1)  # per sample standardisation\n",
    "\n",
    "num_workers = 0\n",
    "pin_memory = True\n",
    "\n",
    "if num_workers > 0:\n",
    "    import cv2\n",
    "    cv2.setNumThreads(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c1252",
   "metadata": {},
   "source": [
    "### TESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96717b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPUS:\n",
    "    path = \"/state/partition1/mmorvan/data/TESS/lightcurves/0001\"\n",
    "else:\n",
    "    path = \"/Users/mario/data/TESS/lightcurves/0027\"\n",
    "\n",
    "train_path = os.path.join(path, 'processed_train')\n",
    "test_path = os.path.join(path, 'processed_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cdce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.kepler_tess import TessDataset, Subset, split_indices\n",
    "from transforms import Compose,StandardScaler, AddGaussianNoise, Mask, FillNans, RandomCrop, DownSample\n",
    "from utils.loading import CollatePred\n",
    "\n",
    "\n",
    "    \n",
    "transform_both_train = Compose([RandomCrop(800, exclude_missing_threshold=0.8),\n",
    "                          DownSample(2),\n",
    "                          Mask(0.3, block_len=None, value=None, exclude_mask=True),\n",
    "                          StandardScaler(dim=0),\n",
    "                          #FillNans(0),\n",
    "                         ])\n",
    "\n",
    "transform_both_2 = Compose([RandomCrop(800, exclude_missing_threshold=0.8),\n",
    "                               DownSample(2),\n",
    "#                                Mask(0.3, block_len=None, value=None, exclude_mask=True),\n",
    "                               StandardScaler(dim=0),\n",
    "                          #FillNans(0),\n",
    "                         ])\n",
    "\n",
    "transform = None\n",
    "\n",
    "if GPUS:\n",
    "    path = \"/state/partition1/mmorvan/data/TESS/lightcurves/0001\"\n",
    "else:\n",
    "    path = \"/Users/mario/data/TESS/lightcurves/0027\"\n",
    "\n",
    "dataset = TessDataset(train_path, \n",
    "                      load_processed=True, \n",
    "                      max_samples=max_samples,\n",
    "                      transform=transform,\n",
    "                      transform_both=transform_both_train,\n",
    "                      use_cache=True,\n",
    "                      )\n",
    "test_dataset = TessDataset(test_path, \n",
    "                           load_processed=True, \n",
    "                           use_cache=True,\n",
    "                          )\n",
    "\n",
    "\n",
    "#dataset.n_dim = 1\n",
    "# TRAIN/VAL SPLIT\n",
    "val_size = int(val_ratio * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_indices, val_indices = split_indices((train_size, val_size),\n",
    "                                           generator=torch.Generator().manual_seed(seed))\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "\n",
    "val_dataset1 = Subset(dataset, val_indices)\n",
    "val_dataset2 = Subset(dataset, val_indices, replace_transform_both=transform_both_2)\n",
    "\n",
    "test_dataset1 = Subset(test_dataset, replace_transform_both=transform_both_train)\n",
    "test_dataset2 = Subset(test_dataset, replace_transform_both=transform_both_2)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=num_workers, pin_memory=pin_memory)\n",
    "val_loader1 = DataLoader(val_dataset1, batch_size=batch_size, shuffle=False, \n",
    "                          num_workers=num_workers, pin_memory=pin_memory)\n",
    "val_loader2 = DataLoader(val_dataset2, batch_size=batch_size, shuffle=False, \n",
    "                          num_workers=num_workers, pin_memory=pin_memory)\n",
    "test_loader1 = DataLoader(test_dataset1, batch_size=batch_size, shuffle=False, \n",
    "                          num_workers=num_workers, pin_memory=pin_memory)\n",
    "test_loader2 = DataLoader(test_dataset2, batch_size=batch_size, shuffle=False, \n",
    "                          num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "loader_pred = DataLoader(test_dataset, \n",
    "                         batch_size=1, \n",
    "                         shuffle=False, \n",
    "                         collate_fn=CollatePred(400, step=350),\n",
    "                         num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07700a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, m, meta = dataset[0]\n",
    "\n",
    "plt.plot(x)\n",
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec782567",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stitching back\n",
    "# X_o = inverse_standardise_batch(X, I['mu'], I['sigma'])\n",
    "\n",
    "# plt.figure(figsize=(25,5))\n",
    "# for i in range(len(X)):\n",
    "#     x = X_o[i]\n",
    "#     plt.plot(range(I['left_crop'][i], I['left_crop'][i]+400),\n",
    "#              x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2213765",
   "metadata": {},
   "source": [
    "### Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import DummyDataset\n",
    "\n",
    "# dataset = DummyDataset(100)\n",
    "# x, y, m, meta = dataset[0]\n",
    "\n",
    "# plt.plot(x)\n",
    "# plt.plot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8878fd0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad952cdc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ### TEST RUN\n",
    "# lit_model = LitImputer(1, noise_scaling=True)\n",
    "# trainer = pl.Trainer(max_epochs=10, gpus=GPUS)\n",
    "# result = trainer.fit(lit_model, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53defe16",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75596030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import LitImputer\n",
    "pl.seed_everything(0)\n",
    "if GPUS:\n",
    "    print_cuda_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitImputer(n_dim=1, d_model=64, dim_feedforward=128, lr=0.001,\n",
    "                       train_unit = 'noise', train_loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592762b5",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd1d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "logger = NeptuneLogger(project=\"denoising-transformer\",\n",
    "                        name='tess_denoising',\n",
    "                       tags=[str(len(dataset))+' samples',\n",
    "                             #\"continued_from_den-186\"\n",
    "                             \"test fix Masked Loss\",\n",
    "                             \"train - \" + lit_model.train_unit,\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d373140",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=5, \n",
    "                     logger=logger, \n",
    "                     gpus=GPUS,\n",
    "                     check_val_every_n_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34c45e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = trainer.fit(lit_model, \n",
    "                     train_dataloaders=train_loader,\n",
    "                     val_dataloaders=[val_loader1, val_loader2], \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca94d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.test(lit_model, dataloaders=[test_loader1, val_loader2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec4b090",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f3c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "num_study = 299\n",
    "\n",
    "ckpt_paths = glob.glob(f\"./.neptune/tess_denoising/DEN-{num_study}/checkpoints/*.ckpt\")\n",
    "assert len(ckpt_paths) == 1\n",
    "ckpt_path = ckpt_paths[0]\n",
    "print(f'successfully found ckpt file for study {num_study}: ', ckpt_path)\n",
    "\n",
    "# ckpt_path = \"./.neptune/Tess-denoising17-02-2022_01-34-35/DEN-167/checkpoints/epoch=382-step=4992.ckpt\"\n",
    "# ckpt_path = \"./.neptune/Tess-denoising_17-02-2022_02-19-37/DEN-170/checkpoints/epoch=1032-step=15494.ckpt\"\n",
    "# ckpt_path = \"./.neptune/Tess-denoising17-02-2022_11-11-26/DEN-186/checkpoints/epoch=601-step=15062.ckpt\"\n",
    "# ckpt_path = \"./.neptune/Tess-denoising17-02-2022_11-41-36/DEN-189/checkpoints/epoch=636-step=15924.ckpt\"\n",
    "# ckpt_path = \"./.neptune/tess_denoising/DEN-219/checkpoints/epoch=2999-step=86999.ckpt\"\n",
    "# ckpt_path = './.neptune/tess_denoising/DEN-245/checkpoints/epoch=1382-step=20744.ckpt'\n",
    "# ckpt_path = \"./.neptune/tess_denoising/DEN-258/checkpoints/epoch=2999-step=44999.ckpt\"\n",
    "# ckpt_path = './.neptune/tess_denoising/DEN-264/checkpoints/epoch=2999-step=44999.ckpt'\n",
    "# ckpt_path = \"./.neptune/tess_denoising/DEN-272/checkpoints/epoch=4999-step=74999.ckpt\"  # MAE\n",
    "# ckpt_path = \"./.neptune/tess_denoising/DEN-289/checkpoints/epoch=4999-step=74999.ckpt\"\n",
    "# ckpt_path = \"./.neptune/tess_denoising/DEN-294/checkpoints/epoch=4999-step=74999.ckpt\"\n",
    "# ckpt_path = \"./.neptune/tess_denoising/DEN-295/checkpoints/epoch=1696-step=25454.ckpt\"\n",
    "# ckpt_path = \"./.neptune/tess_denoising/DEN-296/checkpoints/epoch=4300-step=64514.ckpt\"\n",
    "# ckpt_path = \"./.neptune/tess_denoising/DEN-299/checkpoints/epoch=4999-step=74999.ckpt\"   # MAE + Geom mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc086e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = lit_model.load_from_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf09c4",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96d3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cuda_summary()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from utils.stats import estimate_noise\n",
    "from utils.postprocessing import compute_rollout_attention\n",
    "lit_model.eval().cuda()\n",
    "\n",
    "if 'X' in globals() and 'Y' in globals() and 'M' in globals() and 'I' in globals():\n",
    "    del X, Y, M, I, AR\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, Y, M, I = next(iter(loader_pred))\n",
    "    Y_pred = lit_model(X.cuda()).detach().cpu().numpy()\n",
    "    noise = estimate_noise(Y)\n",
    "    attention_maps = lit_model.get_attention_maps(X.cuda(), mask=M.cuda())\n",
    "    AR = compute_rollout_attention(attention_maps)\n",
    "print('N stars with white noise contribution smaller than 0.5:',(noise <= 0.5).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2251b93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### # plot diagnostic\n",
    "from utils.postprocessing import plot_pred_diagnostic\n",
    "\n",
    "#### All indices in their order\n",
    "indices = range(len(X))\n",
    "step = 1\n",
    "indices = indices[::step]\n",
    "\n",
    "##### Ordered selection wrt to noise contribution\n",
    "# n_plots = 8 \n",
    "# indices = np.argsort(noise.squeeze().detach().cpu())\n",
    "# step = len(indices)//n_plots\n",
    "# step = 2\n",
    "# indices = indices[::step]\n",
    "\n",
    "\n",
    "## Just a random sample\n",
    "# indices = [np.random.randint(len(X))]\n",
    "\n",
    "for i in indices:\n",
    "    x = X[i,:,0].detach().cpu().numpy()\n",
    "    y = Y[i,:,0].detach().cpu().numpy()\n",
    "    mask = M[i,:,0].detach().cpu().numpy()\n",
    "    info = {k:v[i].detach().cpu().item() for k,v in I.items()}\n",
    "    y_pred = Y_pred[i,:,0]\n",
    "    ar = AR[i].sum(1).detach().cpu().numpy()\n",
    "    plot_pred_diagnostic(x, y, y_pred, mask=mask, ar=ar, targetid=info['targetid'], mu=info['mu'], sigma=info['sigma'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc96629",
   "metadata": {},
   "source": [
    "### Plotting examples of attention\n",
    "Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot only attention - pred\n",
    "\n",
    "ncols = 4\n",
    "nrows = 4\n",
    "n = ncols * nrows\n",
    "indices = np.argsort(noise.squeeze().detach().cpu())\n",
    "step = len(indices)//n\n",
    "step = 2\n",
    "indices = indices[::step]\n",
    "\n",
    "fig, ax = plt.subplots(nrows, ncols, figsize=(10,5), sharex=True)\n",
    "fig.add_subplot(111, frameon=False)\n",
    "# hide tick and tick label of the big axes\n",
    "plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "plt.grid(False)\n",
    "plt.ylabel('standardised flux')\n",
    "plt.xlabel('time steps')\n",
    "\n",
    "k = 0\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "        i = indices[k]\n",
    "        x = X[i,:,0].detach().cpu().numpy()\n",
    "        y = Y[i,:,0].detach().cpu().numpy()\n",
    "        mask = M[i,:,0].detach().cpu().numpy()\n",
    "        info = {k:v[i].detach().cpu().item() for k,v in I.items()}\n",
    "        y_pred = Y_pred[i,:,0]\n",
    "        ar = AR[i].sum(1).detach().cpu().numpy()\n",
    "#         ax[row, col].set_title('Prediction')\n",
    "#         ax[row, col].set_ylabel('stand. flux')\n",
    "        ma, Ma = np.min(ar), np.max(ar)\n",
    "        alpha = (ar-ma)/(Ma-ma)/1.002 + ma + 1e-5\n",
    "        s = (((ar-ma)/(Ma-ma)+ma)) * 20 + 1\n",
    "        \n",
    "        ax[row, col].scatter(range(len(x)), y, label='input',\n",
    "                         color='black', s=s, alpha=alpha)\n",
    "\n",
    "        ax[row, col].plot(y_pred, label='pred', color='red', lw=1, alpha=0.9)\n",
    "        ax[row, col].set_yticks([])\n",
    "        k += 1\n",
    "\n",
    "# hide tick and tick label of the big axes\n",
    "#plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "        \n",
    "import datetime\n",
    "date = datetime.datetime.now()\n",
    "plt.savefig(f'experiments/outputs/attention_preds_{date}.pdf', format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870794e9",
   "metadata": {},
   "source": [
    "# Baselines and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50ca31d",
   "metadata": {},
   "source": [
    "### Batch eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b5bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cuda_summary()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from utils.stats import estimate_noise\n",
    "from utils.postprocessing import compute_rollout_attention\n",
    "\n",
    "lit_model.eval().cuda()\n",
    "\n",
    "if 'X' in globals() and 'Y' in globals() and 'M' in globals() and 'I' in globals():\n",
    "    del X, Y, M, I, AR\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, Y, M, I = next(iter(test_loader1))\n",
    "    Y_pred = lit_model(X.cuda()).cpu()\n",
    "    noise = estimate_noise(Y)\n",
    "    attention_maps = lit_model.get_attention_maps(X.cuda(), mask=M.cuda())\n",
    "    AR = compute_rollout_attention(attention_maps)\n",
    "print('N stars with white noise contribution smaller than 0.5:',(noise <= 0.5).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e24fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.postprocessing import inverse_standardise_batch, detrend\n",
    "from utils.stats import naniqr, compute_dw\n",
    "\n",
    "Y_o = inverse_standardise_batch(Y, I['mu'], I['sigma'])\n",
    "Y_pred_o = inverse_standardise_batch(Y_pred, I['mu'], I['sigma'])\n",
    "Y_d = detrend(Y_o, Y_pred_o).numpy()\n",
    "\n",
    "iqr_dtst = naniqr(Y_d, dim=1)\n",
    "dw_dtst = compute_dw(Y_d-1, reduction='none')\n",
    "np.nanmean(iqr_dtst), np.nanmedian(iqr_dtst), np.nanmean(np.abs(2-dw_dtst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae39831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wotan\n",
    "from models import predict_batch_wotan\n",
    "\n",
    "for window in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    print('\\t', window)\n",
    "    Y_wotan_d, _ = predict_batch_wotan(Y_o.squeeze()[:-2], window_length=window)\n",
    "    iqr_wotan = naniqr(Y_wotan_d, dim=1)\n",
    "    dw_wotan = compute_dw(Y_wotan_d-1, reduction='none')\n",
    "    print(iqr_wotan.mean(), np.median(iqr_wotan),  np.abs(2-dw_wotan).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78722574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting IQR scores\n",
    "# plt.hist(iqr_dtst.flatten(), 50, range=(0, 0.05))\n",
    "# plt.hist(iqr_wotan.flatten(), 50, range=(0, 0.05))\n",
    "\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a4838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### plotting baseline\n",
    "# Y_wotan_d, trend_wotan = predict_batch_wotan(Y_o, window_length=0.2)\n",
    "\n",
    "# i = np.random.randint(len(Y))\n",
    "# plt.plot(Y_o[i])\n",
    "# plt.plot(trend_wotan[i])\n",
    "\n",
    "# plot_pred_diagnostic(Y_o[i], Y_o[i], trend_wotan[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ccaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a range of predictions\n",
    "\n",
    "for i in indices:\n",
    "    x = X[i,:,0].detach().cpu().numpy()\n",
    "    y = Y[i,:,0].detach().cpu().numpy()\n",
    "    mask = M[i,:,0].detach().cpu().numpy()\n",
    "    info = {k:v[i].detach().cpu().item() for k,v in I.items()}\n",
    "    y_pred = Y_pred[i,:,0]\n",
    "\n",
    "    y_o = inverse_standardise_batch(y, info['mu'], info['sigma'])\n",
    "    y_pred_o = inverse_standardise_batch(y_pred, info['mu'], info['sigma'])\n",
    "    y_d = detrend(y_o, y_pred_o)\n",
    "    \n",
    "    time = np.arange(len(y_d)) / 24\n",
    "    y_d_biweight = wotan.flatten(time, y_o, method='biweight', return_trend=False)\n",
    "    y_d_medfilt = wotan.flatten(time, y_o, window_length=49, method='medfilt', return_trend=False)\n",
    "    \n",
    "    print(f'IQR(biweight) : {naniqr(y_d_biweight):.5f}')\n",
    "    print(f'IQR(medfilt) : {naniqr(y_d_medfilt):.5f}')\n",
    "    print(f'IQR(dtst) : {naniqr(y_d):.5f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d4170",
   "metadata": {},
   "source": [
    "### Full-LC predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4aed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X, Y, M, I = next(iter(loader_pred))\n",
    "    Y_pred = lit_model(X.cuda()).cpu()\n",
    "    Y_o = inverse_standardise_batch(Y, I['mu'], I['sigma'])\n",
    "    Y_pred_o = inverse_standardise_batch(Y_pred, I['mu'], I['sigma'])\n",
    "    Y_d = detrend(Y_o, Y_pred_o).numpy()\n",
    "\n",
    "# access original TS non transformed \n",
    "idx = I['idx'][0]\n",
    "Y_intact = test_dataset.get_pretransformed_sample(idx)\n",
    "\n",
    "from utils.postprocessing import fold_back\n",
    "# check same as inverse transfo \n",
    "skip = 25\n",
    "seq_len = len(Y_intact)\n",
    "Y_of = fold_back(Y_o, skip, seq_len=seq_len)\n",
    "assert np.isclose(Y_intact.flatten(), Y_of.flatten(), equal_nan=True).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e09f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show full pred with wotan pred too\n",
    "\n",
    "Y_pred_of = fold_back(Y_pred_o, skip, seq_len=seq_len)\n",
    "\n",
    "Y_wotan_d, trend_wotan_of = wotan.flatten(np.arange(seq_len) / 48,\n",
    "                                          Y_of,\n",
    "                                          window_length=0.5,\n",
    "                                          return_trend = True\n",
    "                                          )\n",
    "plt.figure(figsize=(25,10))\n",
    "plt.plot(Y_of)\n",
    "plt.plot(Y_pred_of)\n",
    "plt.plot(trend_wotan_of)\n",
    "# plt.plot(Y_of/Y_pred_of)\n",
    "#plt.xlim(3000,7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0535a5d6",
   "metadata": {},
   "source": [
    "### Full test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ef114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from utils.postprocessing import eval_full_inputs\n",
    "\n",
    "iqr, dw = eval_full_inputs(lit_model, loader_pred, test_dataset, 25, 'cuda')\n",
    "print(iqr, dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d2125",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Produce full predictions for baselines\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "d_pred = {#'dtst': [],\n",
    "          'biweight_0.5': [],\n",
    "          'medfilt': [],\n",
    "            }\n",
    "d_time = {#'dtst': [],\n",
    "          'biweight_0.5': [],\n",
    "          'medfilt': [],\n",
    "            }\n",
    "\n",
    "k = 0\n",
    "for X, Y, M, I in tqdm(loader_pred):\n",
    "    k+=1\n",
    "#     if k> 20:\n",
    "#         break\n",
    "    # access original TS non transformed \n",
    "    idx = I['idx'][0]\n",
    "    Y_intact = test_dataset.get_pretransformed_sample(idx).squeeze()\n",
    "    seq_len = len(Y_intact)\n",
    "    time = np.arange(seq_len) / 48  # Get the actual time vector maybe\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "#         t0 = datetime.now()\n",
    "#         Y_pred = lit_model(X.cuda()).cpu()\n",
    "#         Y_pred_o = inverse_standardise_batch(Y_pred, I['mu'], I['sigma'])\n",
    "#         Y_pred_of = fold_back(Y_pred_o, skip=25, seq_len=seq_len)\n",
    "#         d_pred['dtst'] += [Y_pred_of]\n",
    "#         d_time['dtst'] += [datetime.now()-t0]\n",
    "        \n",
    "        t0 = datetime.now()\n",
    "        d_pred['biweight_0.5'] += [wotan.flatten(time, Y_intact, window_length=0.5, \n",
    "                                                 method='biweight', return_trend = True)[1]]\n",
    "        d_time['biweight_0.5'] += [datetime.now()-t0]\n",
    "\n",
    "        t0 = datetime.now()\n",
    "        d_pred['medfilt'] += [wotan.flatten(time, Y_intact, window_length=49, \n",
    "                                            method='medfilt', return_trend = True)[1]]\n",
    "        d_time['medfilt'] += [datetime.now() - t0]\n",
    "\n",
    "for model in d_pred:\n",
    "    if len(d_pred[model]):\n",
    "        d_pred[model] = np.vstack(d_pred[model])\n",
    "        \n",
    "for model in d_time:\n",
    "    if len(d_time[model]):\n",
    "        d_time[model] = np.mean(d_time[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07caa693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "\n",
    "target_test = np.vstack([test_dataset.get_pretransformed_sample(idx).squeeze() for idx in range(len(test_dataset))])\n",
    "iqr = dict()\n",
    "dw = dict()\n",
    "\n",
    "for model in ['dtst', 'biweight_0.5', 'medfilt']:\n",
    "    pred_d = target_test / d_pred[model]\n",
    "    iqr[model] = naniqr(pred_d, dim=1, reduction='mean')\n",
    "    dw[model] = np.abs(compute_dw(pred_d-1, axis=1, reduce='none')-2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1c4db",
   "metadata": {},
   "source": [
    "#### Computation efficiency of various attention mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c03e01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # # size study\n",
    "# list_seq_len = [100, 200,  400, 700, 1000, 2000]\n",
    "# # for L in list_seq_len:\n",
    "# #     torch.cuda.empty_cache()\n",
    "# #     lit_model = LitImputer(n_dim, d_model=64, dim_feedforward=128,  num_layers=3, eye=eye, lr=0.001,\n",
    "# #                        normal_ratio=0.2, keep_ratio=0., token_ratio=0.8, attention='linear', seq_len=L\n",
    "# #                       )\n",
    "    \n",
    "# #     dataset.transform_both = Compose([RandomCrop(L),\n",
    "# #                                       StandardScaler(dim=0),\n",
    "# #                                      ])\n",
    "# #     train_loader = DataLoader(dataset, batch_size=64, shuffle=True) \n",
    "\n",
    "# #     trainer = pl.Trainer(max_epochs=3, \n",
    "# #                          gpus=1, profiler='simple')\n",
    "\n",
    "# #     result = trainer.fit(lit_model, \n",
    "# #                          train_dataloaders=train_loader,                     \n",
    "# #                          )\n",
    "\n",
    "# # result_full = [0.16705 , 0.24147, 0.51497, np.nan, np.nan, np.nan] #B256\n",
    "# result_full = [0.2 , 0.27, 0.54, 1.2, np.nan, np.nan] #B64\n",
    "# # result_prob = [ 0.30687, 0.33286, 0.38871, 0.54279, 0.68638, 1.1525 ] #B256\n",
    "# result_prob = [0.47, 0.48, 0.53, 0.64, 0.81, 1.3] #B64\n",
    "\n",
    "# ###result_lin = [0.25, 0.31, 0.46, 0.65, 0.87, 1.53] #B64\n",
    "# result_lin = [0.21, 0.24, 0.32, 0.42, 0.52, 0.87]\n",
    "# plt.plot(list_seq_len, result_full, marker='o', label='Full attention B64')#, marker='-o')\n",
    "# plt.plot(list_seq_len, result_prob, marker='o', label='Prob attention B64')\n",
    "# plt.plot(list_seq_len, result_lin, marker='o', label='Linformer attention B64')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Sequence length')\n",
    "# plt.ylabel('Training epoch time')\n",
    "# print('batch size', train_loader.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf13e9",
   "metadata": {},
   "source": [
    "### Look at noise levels and computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91126bb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from utils import nanstd\n",
    "# X,_,_,_ = next(iter(train_loader))\n",
    "# B, L, D = X.shape\n",
    "\n",
    "\n",
    "# window = 10\n",
    "# n_windows = L // window\n",
    "# X.view(B, n_windows, window).shape\n",
    "\n",
    "# noise = nanstd(X.view(B, n_windows, window), -1, keepdim=True).nanmedian(1, keepdim=True).values\n",
    "\n",
    "# # Samples highlighted by non-white-noise contribution to the variance\n",
    "# plt.figure(figsize=(30,20))\n",
    "# for i in range(len(X)):\n",
    "#     plt.plot(X[i,:,0], alpha=1-noise[i,0,0].item()**1.5, lw=1/noise[i,0,0])\n",
    "# plt.title('Batch highlighted by inverse noise')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Distribution of noise estimates\n",
    "# plt.hist(noise[:,0,0].numpy(), 50)\n",
    "# plt.title('White Noise Level')\n",
    "# plt.show()\n",
    "\n",
    "# # After correction\n",
    "# plt.figure(figsize=(30,20))\n",
    "# plt.plot((X / noise)[:,:,0].T)\n",
    "# plt.show()\n",
    "\n",
    "# # Check of better noise estimate\n",
    "\n",
    "# def rolling_std(x, width=10):\n",
    "#     return pd.Series(x).rolling(width, center=True, min_periods=1).std().values\n",
    "\n",
    "# better_noise = []\n",
    "# for i in range(len(X)):\n",
    "#     better_noise += [np.nanmedian(rolling_std(X[i,:,0].numpy(), width=10))]\n",
    "# plt.scatter(noise, better_noise)\n",
    "\n",
    "# # dataset_temp = TessDataset(train_path)\n",
    "\n",
    "\n",
    "# # x, y, m, i = dataset_temp[np.random.randint(len(dataset_temp))]\n",
    "# # plt.plot((x - np.nanmedian(x))/np.nanmedian(x))\n",
    "# pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a1488",
   "metadata": {},
   "source": [
    "#### Attention visu dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # attention_maps = lit_model.get_attention_maps(X.cuda(), mask=M.cuda())\n",
    "# # ar = compute_rollout_attention(attention_maps)\n",
    "\n",
    "\n",
    "\n",
    "# plt.show()\n",
    "# plt.plot(ar)#.sum(0))\n",
    "\n",
    "# plt.show()\n",
    "# for l in range(len(attention_maps)):\n",
    "#     plt.plot(attention_maps[l][i].mean(0).detach().cpu())\n",
    "\n",
    "# chunk = range(290, 295)\n",
    "# attention_maps[0][i, chunk].shape\n",
    "# plt.show()\n",
    "# for l in range(len(attention_maps)):\n",
    "#     plt.plot(attention_maps[l][i, :, chunk].mean(-1).cpu().detach())\n",
    "# plt.show()\n",
    "# for l in range(len(attention_maps)):\n",
    "#     plt.plot(attention_maps[l][i, chunk].mean(0).cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff21c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.stats import estimate_noise\n",
    "\n",
    "# X,_,_,_ = next(iter(train_loader))\n",
    "\n",
    "# noise = estimate_noise(X, reduce='nanmedian')\n",
    "# # plt.hist(noise[9].flatten().numpy(), 50)\n",
    "\n",
    "# print(torch.isnan(X/noise).sum(), torch.isnan(X).sum())\n",
    "# # plt.plot((X/np.sqrt(noise))[:,:,0].T)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e1c08302b48d315bee1cf8c8a590d46ae916d527a5787d1c0b0275f91d47b8b"
  },
  "kernelspec": {
   "display_name": "Python [conda env:dtst]",
   "language": "python",
   "name": "conda-env-dtst-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
